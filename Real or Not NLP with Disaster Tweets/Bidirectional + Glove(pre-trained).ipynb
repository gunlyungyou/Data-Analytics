{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "stop=set(stopwords.words('english'))\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dropout,Dense,SpatialDropout1D, Bidirectional,GlobalMaxPool1D, CuDNNLSTM\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/ASUS/Documents/kaggle data/Real or Not NLP with Disaster Tweets/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('C:/Users/ASUS/Documents/kaggle data/Real or Not NLP with Disaster Tweets/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL, HTML, punc 제거\n",
    "def remove_URL_HTML_punc(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    html = re.compile(r'<.*?>')\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    text = url.sub(\"\", text)\n",
    "    text = html.sub(\"\", text)\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: remove_URL_HTML_punc(x))\n",
    "test['text'] = test['text'].apply(lambda x: remove_URL_HTML_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization(동사만 원형으로 바꿔줌)\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence_words = sentence.split(' ')\n",
    "    new_sentence_words = []\n",
    "    \n",
    "    for sentence_word in sentence_words:\n",
    "        sentence_word = sentence_word.replace('#', '')\n",
    "        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n",
    "        new_sentence_words.append(new_sentence_word)\n",
    "        \n",
    "    new_sentence = ' '.join(new_sentence_words)\n",
    "    new_sentence = new_sentence.strip()\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : lemmatize_sentence(x))\n",
    "test['text'] = test['text'].apply(lambda x : lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#불용어 제거 & 코퍼스 만들기\n",
    "def remove_stop(df):\n",
    "    corpus = []\n",
    "    for tweet in df['text']:\n",
    "        words = [word for word in word_tokenize(tweet) if (word not in stop)]\n",
    "        corpus.append(words)\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "df = pd.concat([train,test])\n",
    "corpus = remove_stop(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained embedding 불러오기\n",
    "embedding_dict = {}\n",
    "with open('C:/Users/ASUS/Documents/kaggle data/Real or Not NLP with Disaster Tweets/nlp-getting-started/glove.6B.100d.txt', 'r',encoding='UTF8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_dict[word] = vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 형태 만들기\n",
    "MAX_LEN = 50\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus) #각 단어들을 numbering으로 표시\n",
    "tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post',padding = 'post') #모든 문장이 동일한 길이를 갖도록 zeor padding(LSTM의 입력으로 들어가야 하기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uniqye words :  20483\n"
     ]
    }
   ],
   "source": [
    "#각 단어에 번호 부여하기\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Number of uniqye words : ' ,len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20483/20483 [00:00<00:00, 662532.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#모든 단어에 glove embedding vector 지정하기\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6471, 50)\n",
      "Shape if Validation  (1142, 50)\n"
     ]
    }
   ],
   "source": [
    "trian_set = tweet_pad[:train.shape[0]]\n",
    "test_set = tweet_pad[train.shape[0]:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(trian_set, train['target'].values, test_size = 0.15)\n",
    "print('Shape of train', X_train.shape)\n",
    "print(\"Shape if Validation \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           2048400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 128)           84992     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 128)           99328     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,245,721\n",
      "Trainable params: 197,321\n",
      "Non-trainable params: 2,048,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix), input_length = MAX_LEN, trainable = False)\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6471 samples, validate on 1142 samples\n",
      "Epoch 1/30\n",
      " - 17s - loss: 0.6946 - acc: 0.5052 - val_loss: 0.6775 - val_acc: 0.6804\n",
      "Epoch 2/30\n",
      " - 12s - loss: 0.6614 - acc: 0.6888 - val_loss: 0.6458 - val_acc: 0.7259\n",
      "Epoch 3/30\n",
      " - 12s - loss: 0.6128 - acc: 0.7393 - val_loss: 0.5841 - val_acc: 0.7653\n",
      "Epoch 4/30\n",
      " - 12s - loss: 0.5468 - acc: 0.7667 - val_loss: 0.5292 - val_acc: 0.7680\n",
      "Epoch 5/30\n",
      " - 12s - loss: 0.5125 - acc: 0.7701 - val_loss: 0.5074 - val_acc: 0.7723\n",
      "Epoch 6/30\n",
      " - 12s - loss: 0.4975 - acc: 0.7762 - val_loss: 0.4980 - val_acc: 0.7758\n",
      "Epoch 7/30\n",
      " - 12s - loss: 0.4878 - acc: 0.7795 - val_loss: 0.4905 - val_acc: 0.7793\n",
      "Epoch 8/30\n",
      " - 12s - loss: 0.4828 - acc: 0.7806 - val_loss: 0.4857 - val_acc: 0.7837\n",
      "Epoch 9/30\n",
      " - 12s - loss: 0.4790 - acc: 0.7864 - val_loss: 0.4822 - val_acc: 0.7793\n",
      "Epoch 10/30\n",
      " - 12s - loss: 0.4677 - acc: 0.7895 - val_loss: 0.4781 - val_acc: 0.7793\n",
      "Epoch 11/30\n",
      " - 12s - loss: 0.4695 - acc: 0.7901 - val_loss: 0.4756 - val_acc: 0.7785\n",
      "Epoch 12/30\n",
      " - 12s - loss: 0.4625 - acc: 0.7942 - val_loss: 0.4739 - val_acc: 0.7802\n",
      "Epoch 13/30\n",
      " - 12s - loss: 0.4686 - acc: 0.7900 - val_loss: 0.4730 - val_acc: 0.7793\n",
      "Epoch 14/30\n",
      " - 12s - loss: 0.4652 - acc: 0.7886 - val_loss: 0.4717 - val_acc: 0.7785\n",
      "Epoch 15/30\n",
      " - 12s - loss: 0.4564 - acc: 0.7991 - val_loss: 0.4702 - val_acc: 0.7846\n",
      "Epoch 16/30\n",
      " - 12s - loss: 0.4604 - acc: 0.7954 - val_loss: 0.4689 - val_acc: 0.7811\n",
      "Epoch 17/30\n",
      " - 12s - loss: 0.4606 - acc: 0.7917 - val_loss: 0.4689 - val_acc: 0.7890\n",
      "Epoch 18/30\n",
      " - 12s - loss: 0.4548 - acc: 0.7986 - val_loss: 0.4682 - val_acc: 0.7872\n",
      "Epoch 19/30\n",
      " - 12s - loss: 0.4627 - acc: 0.7937 - val_loss: 0.4686 - val_acc: 0.7872\n",
      "Epoch 20/30\n",
      " - 12s - loss: 0.4532 - acc: 0.7957 - val_loss: 0.4654 - val_acc: 0.7872\n",
      "Epoch 21/30\n",
      " - 12s - loss: 0.4498 - acc: 0.7980 - val_loss: 0.4647 - val_acc: 0.7890\n",
      "Epoch 22/30\n",
      " - 12s - loss: 0.4504 - acc: 0.8045 - val_loss: 0.4641 - val_acc: 0.7898\n",
      "Epoch 23/30\n",
      " - 12s - loss: 0.4517 - acc: 0.7957 - val_loss: 0.4645 - val_acc: 0.7890\n",
      "Epoch 24/30\n",
      " - 12s - loss: 0.4471 - acc: 0.8064 - val_loss: 0.4643 - val_acc: 0.7925\n",
      "Epoch 25/30\n",
      " - 12s - loss: 0.4482 - acc: 0.7996 - val_loss: 0.4633 - val_acc: 0.7872\n",
      "Epoch 26/30\n",
      " - 12s - loss: 0.4477 - acc: 0.8005 - val_loss: 0.4633 - val_acc: 0.7890\n",
      "Epoch 27/30\n",
      " - 12s - loss: 0.4492 - acc: 0.7980 - val_loss: 0.4626 - val_acc: 0.7890\n",
      "Epoch 28/30\n",
      " - 12s - loss: 0.4437 - acc: 0.8067 - val_loss: 0.4619 - val_acc: 0.7898\n",
      "Epoch 29/30\n",
      " - 12s - loss: 0.4447 - acc: 0.8050 - val_loss: 0.4621 - val_acc: 0.7872\n",
      "Epoch 30/30\n",
      " - 12s - loss: 0.4463 - acc: 0.8011 - val_loss: 0.4633 - val_acc: 0.7881\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "history= model.fit(X_train, y_train, batch_size = 8, epochs = 30, validation_data = (X_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_set)\n",
    "y_pred=np.round(y_pred).astype(int).reshape(3263)\n",
    "submission = pd.read_csv('C:/Users/ASUS/Documents/kaggle data/Real or Not NLP with Disaster Tweets/nlp-getting-started/sample_submission.csv')\n",
    "submission.target = y_pred\n",
    "submission.to_csv('C:/Users/ASUS/Documents/kaggle data/Real or Not NLP with Disaster Tweets/nlp-getting-started/submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
